{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r00Y1oDSAEnH"
      },
      "source": [
        "# HM1: Logistic Regression.\n",
        "\n",
        "### Name: Amena Akbary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY4MTsdQAEnL"
      },
      "source": [
        "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
        "\n",
        "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vT7exaccAEnL"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# for my models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# for optimization\n",
        "import scipy.optimize as opt\n",
        "# for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D48Mrp_pAEnM"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh4Bh47LAEnN"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LZWgiueAEnN",
        "outputId": "f06ddbc0-fb93-45b2-a61c-0a93944b952f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
              "       3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
              "       8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
              "       3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
              "       1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# !pip install ucimlrepo\n",
        "\n",
        "# from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# # fetch dataset\n",
        "# breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
        "\n",
        "# # data (as pandas dataframes)\n",
        "# X = breast_cancer_wisconsin_diagnostic.data.features\n",
        "# y = breast_cancer_wisconsin_diagnostic.data.targets\n",
        "\n",
        "# # metadata\n",
        "# print(breast_cancer_wisconsin_diagnostic.metadata)\n",
        "\n",
        "# # variable information\n",
        "# print(breast_cancer_wisconsin_diagnostic.variables)\n",
        "\n",
        "file_path = \"/content/data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df['diagnosis'].replace({'B': 1, 'M': -1}, inplace = True)\n",
        "\n",
        "X = df.drop([\"diagnosis\", 'Unnamed: 32', 'id'], axis = 1).to_numpy()\n",
        "y = df[\"diagnosis\"].to_numpy()\n",
        "\n",
        "X[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqKmy49qAEnN"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O9XjDMWfAEnO"
      },
      "outputs": [],
      "source": [
        "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant).\n",
        "# You need to get rid of the ID number feature.\n",
        "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "\n",
        "# transform target labels\n",
        "# label_encoder = LabelEncoder() # function would convert categorical data into numerical data\n",
        "# y = label_encoder.fit_transform(y)\n",
        "# y = np.where(y == 0, 1, -1)\n",
        "# y = y.replace()\n",
        "# X = X.to_numpy()\n",
        "# y = y.to_numpy()\n",
        "\n",
        "# print(X.shape)\n",
        "# print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhoZv2WTAEnO"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KCAbhpPbAEnO"
      },
      "outputs": [],
      "source": [
        "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
        "# X = X.to_numpy(dtype='float16')\n",
        "# y = y.to_numpy(dtype='float16')\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "# , test_size = 0.2, random_state = 42\n",
        "# random_state=42 is used to make sure data split is the same every time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ekVOPAkvayC",
        "outputId": "99a678c6-6b5d-425f-8026-242fdd12403f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.029e+00, 1.733e+01, 5.879e+01, ..., 1.750e-01, 4.228e-01,\n",
              "        1.175e-01],\n",
              "       [2.109e+01, 2.657e+01, 1.427e+02, ..., 2.903e-01, 4.098e-01,\n",
              "        1.284e-01],\n",
              "       [9.173e+00, 1.386e+01, 5.920e+01, ..., 5.087e-02, 3.282e-01,\n",
              "        8.490e-02],\n",
              "       ...,\n",
              "       [1.429e+01, 1.682e+01, 9.030e+01, ..., 3.333e-02, 2.458e-01,\n",
              "        6.120e-02],\n",
              "       [1.398e+01, 1.962e+01, 9.112e+01, ..., 1.827e-01, 3.179e-01,\n",
              "        1.055e-01],\n",
              "       [1.218e+01, 2.052e+01, 7.722e+01, ..., 7.431e-02, 2.694e-01,\n",
              "        6.878e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIueem9IAEnP"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtalaId5AEnP"
      },
      "source": [
        "Use the standardization to trainsform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6sZ8Kl8AEnP",
        "outputId": "fdbd7116-2589-442b-ff83-959dbf7b5515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test mean = \n",
            "[ 0.01364641  0.12253485  0.01783047  0.00720095  0.22116437  0.06872354\n",
            " -0.0062141   0.08392701  0.01148532  0.02830473  0.05569554  0.13066342\n",
            "  0.01883126  0.02811456  0.08484284 -0.04224439 -0.14455254 -0.07764354\n",
            " -0.0190853  -0.04580865  0.03540037  0.1165997   0.02366885  0.03165322\n",
            "  0.18107067  0.04910004 -0.05741471  0.03243137 -0.03375599  0.02182886]\n",
            "test std = \n",
            "[0.98431345 1.03563505 0.996237   0.95775565 1.03190362 1.03139701\n",
            " 1.01679871 1.09239913 0.98772373 0.89265057 0.89195878 1.08292998\n",
            " 0.87565931 0.79961637 0.90585062 0.79552757 0.62870432 0.89601551\n",
            " 1.05733888 0.71264305 1.02318884 1.06105793 1.03449232 1.01541716\n",
            " 0.93186461 1.07284763 0.98118663 1.03133888 0.89461526 1.05920895]\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = np.mean(x_train, axis=0).reshape(1, d)\n",
        "sig = np.std(x_train, axis=0).reshape(1, d)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(np.mean(x_test, axis=0))\n",
        "\n",
        "print('test std = ')\n",
        "print(np.std(x_test, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dSnPMmBnrf8",
        "outputId": "78a869e1-a085-4696-bb19-53d00775e97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.44075152 -0.43531903 -1.36208361 ...  0.93201147  2.09724007\n",
            "   1.88644826]\n",
            " [ 1.97409422  1.73302404  2.09166958 ...  2.6989442   1.89115864\n",
            "   2.49783598]\n",
            " [-1.39998062 -1.24962103 -1.34520792 ... -0.97023796  0.59760132\n",
            "   0.05789415]\n",
            " ...\n",
            " [ 0.04880187 -0.55500031 -0.0651254  ... -1.23903241 -0.70863793\n",
            "  -1.27145348]\n",
            " [-0.03896881  0.10207335 -0.03137403 ...  1.05001131  0.43432141\n",
            "   1.21336085]\n",
            " [-0.54860502  0.3132756  -0.60350094 ... -0.61102805 -0.33452086\n",
            "  -0.8462866 ]]\n"
          ]
        }
      ],
      "source": [
        "print(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_uu0rcKAEnP"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gjdAa5z_AEnQ"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 vector\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 vector\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "\n",
        "def objective(w, x, y, lam):\n",
        "    n = len(y) # computing the number of samples in the data set and the length of the target vector y\n",
        "    # print(n)\n",
        "    # print(x.shape)\n",
        "    # print(y.shape)\n",
        "    # print(w.shape)\n",
        "    # calculating the logistic loss\n",
        "    # print(-y * np.dot(x,w))\n",
        "    y = y.reshape(n, 1)\n",
        "    logistic_loss = np.log(1 + np.exp(-y * (x @ w))).mean()\n",
        "      # np.dot(x,w) = computing dot product of feature matrix x and weight vec w\n",
        "      # y* np.dot(x,w) = element wise product of the true labels y and the predictions\n",
        "      # np.exp(-y * ...) = compute expoential\n",
        "      # np.log(1 + np.exp ...) compute logistc loss for each sample\n",
        "\n",
        "    # calculating the L2 lregularization term\n",
        "    l2_regularization = 0.5 * lam * np.linalg.norm(w)**2\n",
        "      # mp.linalg.norm(w) = euclidean norm of w vector\n",
        "\n",
        "    # calculating total objective func value\n",
        "    objective_value = logistic_loss + l2_regularization\n",
        "    return objective_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_initial = np.zeros((d, 1)) # creating initial w vec filled with zeros, (d,1) = shape, d = features\n",
        "test = objective(w_initial, x_train, y_train, 0)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHHOCgJ5cl4O",
        "outputId": "34eb9168-9d17-4132-af84-b1ae83161499"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgt3Vi1efO82",
        "outputId": "7ad81897-a42b-4b42-9a65-8b110c8a8bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of w_initial: (30, 1)\n",
            "Initial objective function value = 0.6931471805599453\n"
          ]
        }
      ],
      "source": [
        "# Initialize w to have a stand point for optimization\n",
        "d = X.shape[1]  # Number of features\n",
        "w_initial = np.zeros((d, 1)) # creating initial w vec filled with zeros, (d,1) = shape, d = features\n",
        "\n",
        "print(\"Shape of w_initial:\", w_initial.shape)\n",
        "\n",
        "# Evaluate the objective function value at w_initial\n",
        "lam = 1E-6  # Regularization parameter 1 X 10^-6\n",
        "objval0 = objective(w_initial, X, y, lam)\n",
        "print('Initial objective function value =', objval0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH_Onh2eAEnQ"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MgnQrjKAEnQ"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZcoHSAeAEnR"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FyycMqBXAEnR"
      },
      "outputs": [],
      "source": [
        "# Calculate the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "    n = len(y)\n",
        "    y = y.reshape(n, 1)\n",
        "\n",
        "    logistic_func = 1 / (1 + np.exp(y * np.dot(x,w)))\n",
        "    # gradient without regularization\n",
        "    gradient_wo_reg = -((x * y) * logistic_func).mean(axis=0)\n",
        "    # logistic_func will reshape to be a column vect to allow multiplication with y\n",
        "    # adding a regularization term\n",
        "    reg_term = lam * w.reshape(-1) # computes L2 reg form\n",
        "    # total gradient\n",
        "    g = gradient_wo_reg + reg_term\n",
        "    return g.reshape(d, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient(w_initial, x_train, y_train, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPfYkeF1dt59",
        "outputId": "b1e4bb36-d3ad-4a6c-d398-697598a0d0cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.34696311],\n",
              "       [ 0.2011086 ],\n",
              "       [ 0.3536242 ],\n",
              "       [ 0.335897  ],\n",
              "       [ 0.18126085],\n",
              "       [ 0.28534753],\n",
              "       [ 0.33014281],\n",
              "       [ 0.3759743 ],\n",
              "       [ 0.16820821],\n",
              "       [-0.00691342],\n",
              "       [ 0.2609813 ],\n",
              "       [-0.00155863],\n",
              "       [ 0.25570766],\n",
              "       [ 0.24983798],\n",
              "       [-0.02808445],\n",
              "       [ 0.12290825],\n",
              "       [ 0.1043909 ],\n",
              "       [ 0.1837267 ],\n",
              "       [ 0.00228194],\n",
              "       [ 0.02016179],\n",
              "       [ 0.37037521],\n",
              "       [ 0.22587957],\n",
              "       [ 0.37446838],\n",
              "       [ 0.34928335],\n",
              "       [ 0.20794578],\n",
              "       [ 0.28454083],\n",
              "       [ 0.31404787],\n",
              "       [ 0.38117855],\n",
              "       [ 0.21275787],\n",
              "       [ 0.15123436]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7iWahaLiAEnR"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 vector, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 vector, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    object_vals = [] # to hold record for each epoch\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "      # cal gradient\n",
        "      # gradient = np.mean(-y * x / (1 + np.exp(y * np.dot(x,w))), axis = 0)[:, np.newaxis] + lam * w\n",
        "      gradients = gradient(w, x, y, lam)\n",
        "      object_val = objective(w, x, y, lam)\n",
        "      object_vals.append(object_val)\n",
        "      # print(gradients.shape)\n",
        "\n",
        "      # update weights\n",
        "      # print(w.shape)\n",
        "      w = w - (gradients * learning_rate)\n",
        "\n",
        "    return w, object_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IMsIgQhAEnR"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6sS9mDTQAEnS"
      },
      "outputs": [],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "def train_log_regress(x_train, y_train):\n",
        "  # initializing weights\n",
        "  w_initial = np.zeros((x_train.shape[1],1)) # x_train.shape[1] = num of features\n",
        "\n",
        "  # setting hyperparameters\n",
        "  lam = 0\n",
        "  learning_rate = 0.01\n",
        "  max_epoch = 100\n",
        "\n",
        "  w_optimal, gd_objvals = gradient_descent(x_train, y_train, lam, learning_rate, w_initial, max_epoch)\n",
        "  # print(\"OBS:\")\n",
        "  # print(objvals)\n",
        "\n",
        "  return w_optimal, gd_objvals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_w_w, gdd_objs = train_log_regress(x_train, y_train)\n",
        "print(w_w_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G23i9v5NxYt",
        "outputId": "a0465278-17c9-440d-d1ff-e4b5b21e6a9e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.15604064]\n",
            " [-0.10333646]\n",
            " [-0.15722531]\n",
            " [-0.14971535]\n",
            " [-0.07013136]\n",
            " [-0.10165262]\n",
            " [-0.12977867]\n",
            " [-0.16139793]\n",
            " [-0.0609515 ]\n",
            " [ 0.02989494]\n",
            " [-0.11148114]\n",
            " [ 0.0045565 ]\n",
            " [-0.10471663]\n",
            " [-0.10531142]\n",
            " [ 0.01768637]\n",
            " [-0.01671764]\n",
            " [-0.00826545]\n",
            " [-0.05286774]\n",
            " [ 0.01090718]\n",
            " [ 0.02827668]\n",
            " [-0.16970763]\n",
            " [-0.11961543]\n",
            " [-0.16852282]\n",
            " [-0.15767151]\n",
            " [-0.09688244]\n",
            " [-0.1125943 ]\n",
            " [-0.12660875]\n",
            " [-0.16501667]\n",
            " [-0.10172351]\n",
            " [-0.05249824]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wO4fBJlVAEnS"
      },
      "outputs": [],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "def train_reg_log_regg(x_train, y_train, lam):\n",
        "  w_initial = np.zeros((x_train.shape[1],1))\n",
        "\n",
        "  learning_rate = 0.1\n",
        "  max_epoch = 100\n",
        "\n",
        "  w_optimal, _ = gradient_descent(x_train, y_train.reshape(-1, 1), lam, learning_rate, w_initial, max_epoch)\n",
        "  # print(y_train.shape)\n",
        "\n",
        "  return w_optimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SndYo-UD9gqt",
        "outputId": "f0900a5f-fd2c-497f-faa4-9f4d7c0cd375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gd_objs\n",
            "[0.6931471805599453, 0.24744125798957817, 0.21256156585536454, 0.19087271340746753, 0.17589970770284602, 0.16480547725713937, 0.15615213828244637, 0.14914679091889954, 0.1433172964010029, 0.13836301290846387, 0.1340818795898875, 0.13033215778631055, 0.12701095194930606, 0.12404143704076574, 0.12136489320994846, 0.11893553846262025, 0.11671706346811066, 0.11468024122387846, 0.11280123744593958, 0.11106039054123508, 0.10944131393702732, 0.10793022445887517, 0.10651543225393346, 0.10518694814131495, 0.10393617763970538, 0.10275567987182516, 0.1016389756489918, 0.10058039327290164, 0.09957494357438207, 0.09861821784008583, 0.09770630382100512, 0.09683571614727425, 0.09600333831154294, 0.09520637401057634, 0.09444230610912877, 0.09370886185213459, 0.09300398322985863, 0.0923258016167895, 0.09167261597401355, 0.09104287403781769, 0.09043515602268824, 0.08984816045095506, 0.08928069178879644, 0.08873164962276564, 0.08820001915517187, 0.0876848628326727, 0.08718531295195656, 0.08670056511069973, 0.08622987239208268, 0.0857725401878434, 0.08532792157876566, 0.08489541320315068, 0.08447445155361114, 0.08406450965077977, 0.08366509404950861, 0.08327574213906153, 0.0828960197038501, 0.08252551871557273, 0.08216385533130811, 0.08181066807528535, 0.08146561618478344, 0.08112837810297001, 0.08079865010353073, 0.08047614503371073, 0.08016059116393003, 0.07985173113347696, 0.07954932098295622, 0.07925312926519498, 0.07896293622721044, 0.07867853305663444, 0.07839972118668673, 0.07812631165440449, 0.07785812450737789, 0.07759498825472398, 0.07733673935845704, 0.0770832217617929, 0.07683428645126225, 0.07658979104980854, 0.07634959943831408, 0.07611358140323829, 0.07588161230826648, 0.07565357278805979, 0.07542934846237051, 0.07520882966894106, 0.07499191121374565, 0.07477849213725941, 0.074568475495553, 0.07436176815511364, 0.07415828060038658, 0.07395792675311476, 0.07376062380263033, 0.07356629204632208, 0.07337485473956395, 0.07318623795444856, 0.07300037044672016, 0.07281718353035059, 0.07263661095924327, 0.07245858881559096, 0.07228305540444867, 0.07210995115411621]\n",
            "wei\n",
            "[[-0.11683315]\n",
            " [-0.08118478]\n",
            " [-0.11750976]\n",
            " [-0.11231396]\n",
            " [-0.05224719]\n",
            " [-0.0723216 ]\n",
            " [-0.09684149]\n",
            " [-0.12226873]\n",
            " [-0.04451725]\n",
            " [ 0.02487354]\n",
            " [-0.08514907]\n",
            " [ 0.00287968]\n",
            " [-0.07866575]\n",
            " [-0.07939083]\n",
            " [ 0.01011315]\n",
            " [-0.00643046]\n",
            " [-0.00022865]\n",
            " [-0.03577892]\n",
            " [ 0.00815421]\n",
            " [ 0.02624955]\n",
            " [-0.12851395]\n",
            " [-0.09481343]\n",
            " [-0.12698217]\n",
            " [-0.11916724]\n",
            " [-0.07640036]\n",
            " [-0.08343922]\n",
            " [-0.09492988]\n",
            " [-0.12469895]\n",
            " [-0.08057982]\n",
            " [-0.03974974]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "weights, gd_objs = gradient_descent(x_train,y_train, 0, 0.5, np.zeros((30, 1)))\n",
        "# print(type(x_train))\n",
        "# print(type(y_train))\n",
        "print(\"gd_objs\")\n",
        "print(gd_objs)\n",
        "\n",
        "wei = train_reg_log_regg(x_train,y_train,1)\n",
        "print(\"wei\")\n",
        "print(wei)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEkigJZcAEnS"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ibHS64yBAEnS"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "    xi = np.array(xi).reshape(-1, 1)\n",
        "    yi = np.array([yi]).reshape(-1, 1)\n",
        "\n",
        "    # Linear term\n",
        "    linear = np.dot(xi.T, w)\n",
        "    linear_clipped = np.clip(linear, -20, 20)\n",
        "\n",
        "    # Logistic loss\n",
        "    logistic_func = 1 / (1 + np.exp(-yi * linear_clipped) + 1e-8)\n",
        "\n",
        "    # Obj with clipping\n",
        "    obj = np.log(1 + np.exp(-yi * linear_clipped)) + 0.5 * lam * np.linalg.norm(w)**2\n",
        "\n",
        "    # Gradient with clipping\n",
        "    gradient_wo_reg = -yi * xi / (logistic_func + 1e-8)\n",
        "\n",
        "    gradient = gradient_wo_reg + lam * w\n",
        "\n",
        "    return np.sum(obj), gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6l-sIeDJs5C1"
      },
      "outputs": [],
      "source": [
        "cat = stochastic_objective_gradient(w_initial, x_train[0], y_train[0], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC1fD_kDyrJu",
        "outputId": "8c2f53d3-b225-44ae-b72c-ee8ae8e6102c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x_train[0].shape\n",
        "# y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaK1mjoNAEnT"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples.\n",
        "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4TD88BO2AEnT"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, w, max_epoch):\n",
        "    n = x.shape[0]\n",
        "    objvals = []\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        sum_obj = 0\n",
        "        perm = np.random.permutation(n)\n",
        "        x_shuffle = x[perm]\n",
        "        y_shuffle = y[perm]\n",
        "\n",
        "        for i in range(n):\n",
        "            xi = x_shuffle[i,:]\n",
        "            yi = y_shuffle[i]\n",
        "\n",
        "            obj, gradient = stochastic_objective_gradient(w, xi, yi, lam)\n",
        "\n",
        "            sum_obj += obj\n",
        "\n",
        "            w = w - learning_rate * gradient\n",
        "\n",
        "        objvals.append(sum_obj / n)\n",
        "\n",
        "\n",
        "    return w, objvals\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cats = sgd(x_train, y_train, 0, 0.0001, w_initial, max_epoch=100)\n",
        "# print(cats[1])"
      ],
      "metadata": {
        "id": "wczaKhcv16Nv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp3IdDqiAEnT"
      },
      "source": [
        "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-XiXSgAEnT",
        "outputId": "3c31f32b-c915-46b7-c451-898a26cbcabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Weights:\n",
            "[[-1.37073732]\n",
            " [-1.63246706]\n",
            " [-1.36282828]\n",
            " [-1.34782311]\n",
            " [-1.27434267]\n",
            " [-0.32551021]\n",
            " [-1.06921636]\n",
            " [-1.86925103]\n",
            " [-0.07880714]\n",
            " [ 0.39866654]\n",
            " [-2.22779707]\n",
            " [-1.06221035]\n",
            " [-1.82111902]\n",
            " [-1.64945311]\n",
            " [ 0.92623462]\n",
            " [ 0.97933349]\n",
            " [ 0.45913744]\n",
            " [-0.96159576]\n",
            " [ 1.09132476]\n",
            " [ 0.96832899]\n",
            " [-1.83016767]\n",
            " [-2.08533134]\n",
            " [-1.70301552]\n",
            " [-1.70807318]\n",
            " [-1.08808069]\n",
            " [-0.49328103]\n",
            " [-1.07642433]\n",
            " [-2.04147139]\n",
            " [-0.42053974]\n",
            " [-0.0889762 ]]\n",
            "Objvals\n",
            "[0.5658603030377372, 0.41061281317157544, 0.3308209485885767, 0.28548800392207835, 0.2579662434509941, 0.24104192174824227, 0.2351985527002541, 0.2340695529807365, 0.23445299236392797, 0.24231116435749528, 0.24953869556562688, 0.2617902192135646, 0.27470045061725595, 0.2867765458646805, 0.30233266793428776, 0.32073233097062037, 0.33532365399730923, 0.3540072287373476, 0.37399207295587295, 0.39164997566701704, 0.41296678864147696, 0.427135005681968, 0.4508891059580243, 0.4672624502233536, 0.4852148557636079, 0.5081677840868979, 0.5342057121071552, 0.5572180244343456, 0.57347753551118, 0.6038183935415655, 0.6197891826563661, 0.644254102606625, 0.6700875351478697, 0.6952352762699598, 0.7220490270594162, 0.7508101449523854, 0.7691346211836256, 0.8021971015930699, 0.8281334210765112, 0.8564491678957367, 0.8767695009990484, 0.9095621438678311, 0.9375394624617989, 0.9654796771840944, 0.9925846962923911, 1.0282316540755385, 1.054607913195939, 1.0807333605577505, 1.112207598586039, 1.1438158979811945, 1.174854300328198, 1.2069641706980674, 1.2320860651384504, 1.2671880947404097, 1.2974159812109471, 1.3305444011993348, 1.3603761330916377, 1.3929966683298274, 1.420561973879232, 1.454699155033735, 1.4862716417579864, 1.5142347611499551, 1.550461502951738, 1.5815313760210712, 1.6116471816493418, 1.647986088281261, 1.6803230822729405, 1.7074686030537274, 1.741450771924641, 1.7749381163339222, 1.806250687671864, 1.8392720427771254, 1.8714811432030272, 1.9040095286547982, 1.9334089677104622, 1.9703531491016677, 2.0038332271893755, 2.0315579390776577, 2.058726102506462, 2.1011044323971215, 2.1249405378245245, 2.1583341712300745, 2.1937574452973885, 2.2243585312997407, 2.244982999377333, 2.281622109848514, 2.317865419081706, 2.3470756743929764, 2.3796102282594234, 2.4061597265964494, 2.4427077512436886, 2.466805674812697, 2.5073833265709498, 2.5259731504238663, 2.561413739123726, 2.5964442525154943, 2.621379248167155, 2.66184044512567, 2.6842263532935373, 2.7134032348342947]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "# Set hyperparameters\n",
        "lam = 0.1  # Regularization parameter\n",
        "learning_rate = 0.0001\n",
        "max_epoch = 100\n",
        "\n",
        "# # print(X[:5])\n",
        "# print(\"Shape of X:\", X.shape)\n",
        "# print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# Train logistic regression using SGD\n",
        "w_w, sgd_objvals = sgd(x_train, y_train, lam, learning_rate, np.zeros((30, 1)), max_epoch)\n",
        "\n",
        "# Print optimal weights\n",
        "print(\"Optimal Weights:\")\n",
        "print(w_w)\n",
        "\n",
        "print(\"Objvals\")\n",
        "print(sgd_objvals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vFxxcOuAEnU",
        "outputId": "e77f3d82-256e-4f07-98d3-f0f16fb43fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Weights:\n",
            "[[-1.37222042]\n",
            " [-1.6343022 ]\n",
            " [-1.36443405]\n",
            " [-1.34924793]\n",
            " [-1.27517611]\n",
            " [-0.32800155]\n",
            " [-1.07120014]\n",
            " [-1.87076613]\n",
            " [-0.07824914]\n",
            " [ 0.39707294]\n",
            " [-2.22688954]\n",
            " [-1.0619502 ]\n",
            " [-1.82074786]\n",
            " [-1.64971685]\n",
            " [ 0.92322626]\n",
            " [ 0.97705625]\n",
            " [ 0.4584091 ]\n",
            " [-0.96116471]\n",
            " [ 1.08932728]\n",
            " [ 0.96680383]\n",
            " [-1.83118185]\n",
            " [-2.08749826]\n",
            " [-1.70420829]\n",
            " [-1.70914745]\n",
            " [-1.09105453]\n",
            " [-0.49504199]\n",
            " [-1.07795108]\n",
            " [-2.04198325]\n",
            " [-0.42211136]\n",
            " [-0.09142034]]\n",
            "Obj Vals\n",
            "[0.5655511924916952, 0.4103805464049415, 0.3301243149384363, 0.2851292087047253, 0.2571236300653884, 0.24248191014520334, 0.23521138656995338, 0.23336190949795968, 0.2347337328591321, 0.24149515083857614, 0.25167465530362815, 0.2606667533038233, 0.27334712745701056, 0.2874229117128186, 0.3042538727164023, 0.32062608918483126, 0.3351349842020027, 0.35395690907895033, 0.3722499336198824, 0.3944420650205978, 0.40843776455787273, 0.4281625314684034, 0.4500848405288435, 0.46980881633360816, 0.4860426178649721, 0.5081280134049121, 0.5302364431516006, 0.5574683545662914, 0.5754305535749186, 0.5995555893078497, 0.6224805864357572, 0.6433240876960629, 0.6720133105985158, 0.6955923119494372, 0.7202266686705473, 0.7494675119530989, 0.7739235871814536, 0.7983136772136764, 0.8228573185888317, 0.8545422040942199, 0.8809570818998604, 0.91103354856586, 0.9412366930324731, 0.9647764338201249, 0.9925614139111048, 1.0208678957785426, 1.0497846440257224, 1.0827369102204005, 1.1117288546115738, 1.141772613104593, 1.1769821626304906, 1.2101218350950063, 1.2318326691990367, 1.2641365115045655, 1.2920830615506531, 1.330005992458506, 1.3613256356285577, 1.3935422783627076, 1.4238353692258274, 1.451870060625998, 1.4891814382159299, 1.5170406096670181, 1.5483127187750891, 1.5811969804585773, 1.6124894873802005, 1.6494167099747779, 1.6765050802622685, 1.7155996024055078, 1.7368509736045419, 1.7804419856613984, 1.8064672098809922, 1.8423259999045847, 1.8707565478876842, 1.9009081829847925, 1.9325931000175105, 1.9664253292503064, 2.0047976587955327, 2.0364058435039554, 2.0599088401250496, 2.098495260847033, 2.1238934367020303, 2.1599606306445756, 2.189032050713955, 2.2202375442497377, 2.2491331344995724, 2.287346723646741, 2.3174902120971574, 2.3493600765478737, 2.3808777358186344, 2.4077872987076123, 2.4377398881203174, 2.4703100281327375, 2.5024956182003733, 2.530658607770829, 2.564343350926775, 2.592929309617293, 2.624103200744107, 2.653854489312471, 2.6865899585396416, 2.7176588874224357]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "# cats = sgd(x_train, y_train, 0, 0.0001, w_initial, max_epoch=100)\n",
        "# print(cats[1])\n",
        "ww, sgd_re_objvals = sgd(x_train, y_train, 0.1, 0.0001, w_initial, max_epoch = 100)\n",
        "\n",
        "print(\"Optimal Weights:\")\n",
        "print(ww)\n",
        "\n",
        "print(\"Obj Vals\")\n",
        "print(sgd_re_objvals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCAXWs2MAEnU"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbXdmqgWAEnU"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YfYF3kZuAEnV"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-b matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "  b = len(yi)\n",
        "  y = yi.reshape(b, 1)\n",
        "  logistic_func = 1 / (1 + np.exp(y * np.dot(xi,w)))\n",
        "    # gradient without regularization\n",
        "  gradient_wo_reg = -((xi * y) * logistic_func).mean(axis=0)\n",
        "    # logistic_func will reshape to be a column vect to allow multiplication with y\n",
        "    # adding a regularization term\n",
        "  reg_term = lam * w.reshape(-1) # computes L2 reg form\n",
        "    # total gradient\n",
        "  g = gradient_wo_reg + reg_term\n",
        "  logistic_loss = np.log(1 + np.exp(-y * (xi @ w))).mean()\n",
        "      # np.dot(x,w) = computing dot product of feature matrix x and weight vec w\n",
        "      # y* np.dot(x,w) = element wise product of the true labels y and the predictions\n",
        "      # np.exp(-y * ...) = compute expoential\n",
        "      # np.log(1 + np.exp ...) compute logistc loss for each sample\n",
        "\n",
        "    # calculating the L2 lregularization term\n",
        "  l2_regularization = 0.5 * lam * np.linalg.norm(w)**2\n",
        "      # mp.linalg.norm(w) = euclidean norm of w vector\n",
        "\n",
        "    # calculating total objective func value\n",
        "  objective_value = logistic_loss + l2_regularization\n",
        "  return objective_value, g.reshape(d,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catss = mb_objective_gradient(np.zeros((30, 1)), x_train, y_train, 0)\n",
        "catss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v46K4Nmr5ZKF",
        "outputId": "1e94bbbf-7c3b-4e25-feec-07bfcee59356"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471805599453,\n",
              " array([[ 0.34696311],\n",
              "        [ 0.2011086 ],\n",
              "        [ 0.3536242 ],\n",
              "        [ 0.335897  ],\n",
              "        [ 0.18126085],\n",
              "        [ 0.28534753],\n",
              "        [ 0.33014281],\n",
              "        [ 0.3759743 ],\n",
              "        [ 0.16820821],\n",
              "        [-0.00691342],\n",
              "        [ 0.2609813 ],\n",
              "        [-0.00155863],\n",
              "        [ 0.25570766],\n",
              "        [ 0.24983798],\n",
              "        [-0.02808445],\n",
              "        [ 0.12290825],\n",
              "        [ 0.1043909 ],\n",
              "        [ 0.1837267 ],\n",
              "        [ 0.00228194],\n",
              "        [ 0.02016179],\n",
              "        [ 0.37037521],\n",
              "        [ 0.22587957],\n",
              "        [ 0.37446838],\n",
              "        [ 0.34928335],\n",
              "        [ 0.20794578],\n",
              "        [ 0.28454083],\n",
              "        [ 0.31404787],\n",
              "        [ 0.38117855],\n",
              "        [ 0.21275787],\n",
              "        [ 0.15123436]]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u10gfZ0AEnV"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
        "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "v23YGOgyAEnW"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "  batch_size = 20\n",
        "  sample_count, feature_count = x.shape\n",
        "  objective_vales = np.zeros(max_epoch)\n",
        "\n",
        "  for epoch in range(max_epoch):\n",
        "    shuffled_indices = np.random.permutation(sample_count)\n",
        "    x_shuffled = x[shuffled_indices,:]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "\n",
        "    total_obj = 0\n",
        "\n",
        "    for start_idx in range(0, sample_count, batch_size):\n",
        "      end_idx = start_idx + batch_size\n",
        "      xi_batch = x_shuffled[start_idx:end_idx, :]\n",
        "      yi_batch = y_shuffled[start_idx:end_idx]\n",
        "\n",
        "\n",
        "      batch_obj, batch_grad = mb_objective_gradient(w, xi_batch, yi_batch, lam)\n",
        "      # batch_grad = batch_grad.reshape(-1)\n",
        "      # print(batch_grad.shape)\n",
        "      # print(w.shape)\n",
        "      total_obj += batch_obj * len(yi_batch)\n",
        "\n",
        "      w -= learning_rate * batch_grad\n",
        "\n",
        "    objective_vales[epoch] = total_obj / sample_count\n",
        "  return w, objective_vales\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS5CchrTAEnW"
      },
      "source": [
        "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v65iXHM3AEnW",
        "outputId": "3c52456e-bd36-40a5-ee9c-f5ba2eba85cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Weights:\n",
            "[[-0.21094664]\n",
            " [-0.15185742]\n",
            " [-0.21149843]\n",
            " [-0.20424982]\n",
            " [-0.08887727]\n",
            " [-0.11813701]\n",
            " [-0.16612769]\n",
            " [-0.21649386]\n",
            " [-0.07288027]\n",
            " [ 0.05600415]\n",
            " [-0.15608017]\n",
            " [ 0.00459088]\n",
            " [-0.14281395]\n",
            " [-0.14694952]\n",
            " [ 0.01927177]\n",
            " [ 0.00517937]\n",
            " [ 0.0168976 ]\n",
            " [-0.04992795]\n",
            " [ 0.02001634]\n",
            " [ 0.06401241]\n",
            " [-0.23364727]\n",
            " [-0.17809577]\n",
            " [-0.22975119]\n",
            " [-0.21827764]\n",
            " [-0.13693511]\n",
            " [-0.14018028]\n",
            " [-0.16126776]\n",
            " [-0.21859898]\n",
            " [-0.14350215]\n",
            " [-0.06116275]]\n",
            "Objective Values:\n",
            "[0.673168   0.6346238  0.60126416 0.57224773 0.54698571 0.52463395\n",
            " 0.50479135 0.4871021  0.47131516 0.45704698 0.44407467 0.43236238\n",
            " 0.42162651 0.41181573 0.40276121 0.39436737 0.38666327 0.37946712\n",
            " 0.37276519 0.3665138  0.36067137 0.35518139 0.35002993 0.3452023\n",
            " 0.34063152 0.336331   0.33226073 0.328395   0.32472262 0.32124936\n",
            " 0.31795771 0.31480871 0.31180773 0.30893451 0.30622964 0.30362033\n",
            " 0.3011085  0.29871679 0.29642035 0.29422923 0.29213052 0.29010002\n",
            " 0.28815354 0.28627564 0.28446298 0.28273038 0.28106317 0.27943701\n",
            " 0.27788833 0.2763862  0.27491815 0.27351653 0.27216413 0.27084993\n",
            " 0.26957587 0.26835451 0.26715508 0.26599958 0.2648731  0.26380057\n",
            " 0.26274647 0.26171925 0.26071968 0.25975312 0.25881942 0.2579094\n",
            " 0.25702487 0.25616756 0.25533091 0.25452983 0.25373021 0.25295274\n",
            " 0.25219697 0.25147307 0.25075162 0.25005445 0.24938232 0.24872483\n",
            " 0.24808445 0.24746158 0.24684763 0.24626381 0.24568106 0.24510736\n",
            " 0.2445581  0.24401724 0.24349622 0.24297411 0.24247146 0.24198797\n",
            " 0.24150401 0.24103894 0.24057943 0.2401271  0.23969042 0.23926935\n",
            " 0.23884697 0.23844176 0.23803544 0.2376422 ]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0.1\n",
        "learning_rate = 0.001\n",
        "max_epoch = 100\n",
        "\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "w_opt, mbgd_objvals = mbgd(x_train, y_train, lam, learning_rate, np.zeros((30, 1)), max_epoch)\n",
        "\n",
        "print(\"Optimal Weights:\")\n",
        "print(w_opt)\n",
        "\n",
        "print(\"Objective Values:\")\n",
        "print(mbgd_objvals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVS8hoyeAEnX",
        "outputId": "29693d33-48be-47ff-9297-de23be752b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Weights:\n",
            "[[-0.26501321]\n",
            " [-0.24222359]\n",
            " [-0.26316438]\n",
            " [-0.27781871]\n",
            " [-0.10024649]\n",
            " [-0.08056944]\n",
            " [-0.23667655]\n",
            " [-0.30853561]\n",
            " [-0.0691492 ]\n",
            " [ 0.08560534]\n",
            " [-0.27367852]\n",
            " [-0.00165271]\n",
            " [-0.22498332]\n",
            " [-0.24707433]\n",
            " [-0.03083635]\n",
            " [ 0.09134034]\n",
            " [ 0.06434724]\n",
            " [-0.02228547]\n",
            " [ 0.03488595]\n",
            " [ 0.12767727]\n",
            " [-0.33020123]\n",
            " [-0.30441151]\n",
            " [-0.31296564]\n",
            " [-0.32659511]\n",
            " [-0.22636808]\n",
            " [-0.14797646]\n",
            " [-0.22188376]\n",
            " [-0.28906896]\n",
            " [-0.24125161]\n",
            " [-0.08705691]]\n",
            "Objective Values:\n",
            "[0.21543424 0.21541656 0.21543103 0.21544063 0.21543231 0.21543263\n",
            " 0.21544177 0.21542255 0.21544479 0.2154566  0.21541528 0.21543685\n",
            " 0.21542467 0.21546779 0.21542037 0.21545178 0.2154416  0.21544144\n",
            " 0.21544384 0.21542394 0.21539736 0.21544719 0.21546452 0.21542624\n",
            " 0.21541003 0.21544474 0.21544309 0.21543639 0.21542098 0.21541887\n",
            " 0.2154377  0.21541165 0.2154478  0.21543081 0.21542435 0.21542013\n",
            " 0.21542431 0.21545607 0.21541362 0.21542003 0.21542586 0.21543078\n",
            " 0.21541419 0.21541929 0.21544625 0.21545196 0.21546088 0.21541284\n",
            " 0.215424   0.21542963 0.21541351 0.21545419 0.21543592 0.21545663\n",
            " 0.21543501 0.21541496 0.21544479 0.2154308  0.21541958 0.21545948\n",
            " 0.21544109 0.21541222 0.21544287 0.21543735 0.21547019 0.21541626\n",
            " 0.2154381  0.21540247 0.21544785 0.21542901 0.21543864 0.21542849\n",
            " 0.21542969 0.21545555 0.21543965 0.21543765 0.21543782 0.2154312\n",
            " 0.21544819 0.21542187 0.21543289 0.2154399  0.21542547 0.21546129\n",
            " 0.21542214 0.21544243 0.21543778 0.21543838 0.2154188  0.2154081\n",
            " 0.21545773 0.21544099 0.21541702 0.21542929 0.21542983 0.21543623\n",
            " 0.21543573 0.21544351 0.21542479 0.21544997]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistric regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lam = 0.1\n",
        "learning_rate = 0.01\n",
        "max_epoch = 100\n",
        "\n",
        "w_optimals, objvals = mbgd(x_train, y_train, lam, learning_rate, w_initial, max_epoch)\n",
        "\n",
        "print(\"Optimal Weights:\")\n",
        "print(w_optimals)\n",
        "print(\"Objective Values:\")\n",
        "print(objvals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWo4QokhAEnX"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "FLrt-pcQT8e5",
        "outputId": "26d33f84-8a0b-4553-eb15-6f0e01bc91d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs = len(gd_objs)\n",
        "x = range(epochs)\n",
        "\n",
        "plt.plot(x, gd_objs, label=\"GD\")\n",
        "plt.plot(x, sgd_objvals, label=\"SGD\")\n",
        "plt.plot(x, mbgd_objvals, label=\"MBGD\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Objective\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87UYJdmAEnY"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TGVYinoSAEnZ"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "def predict(w, X):\n",
        "  z = X @ w # cal product\n",
        "  prob = 1 / (1 + np.exp(-z)) # summation function\n",
        "\n",
        "  f = np.where(prob >= 0, 1, -1) # setting threshold to determine class labels\n",
        "\n",
        "  return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ur_0fbvJPSvY"
      },
      "outputs": [],
      "source": [
        "def get_acc(w, X, y_test):\n",
        "  pre = predict(w, X)\n",
        "  results = (pre == y_test)\n",
        "  acc = np.mean(results)\n",
        "  # for i in range(len(y_test)):\n",
        "  #   if results[i] == True:\n",
        "  #     acc += 1\n",
        "  return acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "L7wdCPzPAEnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa17ccf1-4500-4a6d-e318-348fab6b7786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y test accuracy gd: 0.6228070175438597\n",
            "Y test accuracy sgd 0.6228070175438597\n",
            "Y test accuracy mbgd 0.6228070175438597\n"
          ]
        }
      ],
      "source": [
        "# evaluate training error of logistric regression and regularized version\n",
        "y_pred_train_logistic = predict(weights, x_train)\n",
        "y_pred_test_logistic = predict(wei, x_test)\n",
        "# gets weights for the three models and get accuracy\n",
        "\n",
        "y_acc_test_gd = get_acc(weights, x_test, y_test)\n",
        "print(\"Y test accuracy gd:\", y_acc_test_gd)\n",
        "y_acc_test_sgd = get_acc(w_w, x_test, y_test)\n",
        "print(\"Y test accuracy sgd\", y_acc_test_sgd)\n",
        "y_acc_test_mbgd = get_acc(w_opt, x_test, y_test)\n",
        "print(\"Y test accuracy mbgd\", y_acc_test_mbgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vg3n8w3NAEnZ"
      },
      "outputs": [],
      "source": [
        "# evaluate testing error of logistric regression and regularized version\n",
        "def errors(w, X, y):\n",
        "  acc = get_acc(w, X, y)\n",
        "  error = 1 - acc\n",
        "  return error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_error_test_gf_lr = errors(weights, x_train, y_train)\n",
        "print(\"Error testing logistric regression gd:\", y_error_test_gf_lr)\n",
        "\n",
        "y_error_test_gf_lrr = errors(wei, x_test, y_test)\n",
        "print(\"Error testing logistric regression regularized gd:\", y_error_test_gf_lrr)\n",
        "\n",
        "y_error_test_sgd_lr = errors(w_w, x_train, y_train)\n",
        "print(\"Error testing logistric regression sgd:\", y_error_test_sgd_lr)\n",
        "\n",
        "y_error_test_sgd_lrr = errors(ww, x_test, y_test)\n",
        "print(\"Error testing logistic regression regularized sgd:\", y_error_test_sgd_lrr)\n",
        "\n",
        "y_error_test_mbgd_lr = errors(w_opt, x_train, y_train)\n",
        "print(\"Error testing logistric regression mbgd:\", y_error_test_mbgd_lr)\n",
        "\n",
        "y_error_test_mbgd_lrr = errors(w_optimals, x_test, y_test)\n",
        "print(\"Error testing logistic regression regularized mbgd:\", y_error_test_mbgd_lrr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqKJMZacMuXz",
        "outputId": "e532eeb3-26cb-4dbf-ea4d-d2a10516ca83"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error testing logistric regression gd: 0.37142857142857144\n",
            "Error testing logistric regression regularized gd: 0.3771929824561403\n",
            "Error testing logistric regression sgd: 0.37142857142857144\n",
            "Error testing logistic regression regularized sgd: 0.3771929824561403\n",
            "Error testing logistric regression mbgd: 0.37142857142857144\n",
            "Error testing logistic regression regularized mbgd: 0.3771929824561403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhFoHurKAEna"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jhlRKJCAEna"
      },
      "source": [
        "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing different learning rates & their accuracy\n",
        "#one\n",
        "onew, gdd_o = gradient_descent(x_train,y_train, 0, 0.5, np.zeros((30, 1)))\n",
        "one = get_acc(onew, x_test, y_test)\n",
        "print(\"Y test accuracy gd:\", one)\n",
        "\n",
        "# two\n",
        "oneww, gddd_o = gradient_descent(x_train,y_train, 1, 0.05, np.zeros((30, 1)))\n",
        "onee = get_acc(oneww, x_test, y_test)\n",
        "print(\"Y test accuracy gd:\", onee)\n",
        "\n",
        "# three\n",
        "onewww, gdddd_o = gradient_descent(x_train,y_train, 2, 0.005, np.zeros((30, 1)))\n",
        "oneee = get_acc(onewww, x_test, y_test)\n",
        "print(\"Y test accuracy gd:\", oneee)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs = len(gd_objs)\n",
        "x = range(epochs)\n",
        "\n",
        "plt.plot(x, gdd_o, label=\"ONE\")\n",
        "plt.plot(x, gddd_o, label=\"TWO\")\n",
        "plt.plot(x, gdddd_o, label=\"THREE\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Objective\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "2Ky2ODHrRe48",
        "outputId": "c8633c65-cb6e-4892-988d-eb0daef9f3b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y test accuracy gd: 0.6228070175438597\n",
            "Y test accuracy gd: 0.6228070175438597\n",
            "Y test accuracy gd: 0.6228070175438597\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph I can see that with curve one having learning rate 0.5 and no regularization it converges quickly indicating a sharp decline in the objective function. The curve two with a learning rate of 0.05 and a regularization of 1 converges more slowly and ends at a higher objective values than one. The curve three with a learning rate of 0.0005 and a regularization of 2 shows that the the convergent rate is making the optimization a lot smoother."
      ],
      "metadata": {
        "id": "3fk5SSvQXxyz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}